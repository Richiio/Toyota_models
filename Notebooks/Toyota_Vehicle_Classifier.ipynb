{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toyota_Vehicle_Classifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1xag7zbEBzntHiy7cRt0-e5092UbBxucP",
      "authorship_tag": "ABX9TyPkQE6VdZoMENCY3USnmPah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fortune-Adekogbe/Toyota-Models-Classification/blob/main/Notebooks/Toyota_Vehicle_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVz0zGYvCSK"
      },
      "source": [
        "# **Toyota Vehicle Classification Project**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX3sK3Q4vONt"
      },
      "source": [
        "Setting up the Project Directory and checking its contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk_NX27k3Irx"
      },
      "source": [
        "#!mkdir /content/drive/MyDrive/Colab_Data/FlowerClassificationData\n",
        "%cd /content/drive/MyDrive/Colab_Data/curacel/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wip-UKKe_uYy"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YllXX9Q1vusy"
      },
      "source": [
        "**Importing required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4G5cl3IKqgm"
      },
      "source": [
        "import os, sys, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torchvision import *\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from random import shuffle \n",
        "from shutil import copyfile\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiHq9bA0v00S"
      },
      "source": [
        "### Setting up the seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6cug8HGzwPo"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjMyVY5qwFuY"
      },
      "source": [
        "**Check if pytorch is imported and if GPU is enabled**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNmGF89JTHF"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2W_wTFOMubX"
      },
      "source": [
        "import json\n",
        "import joblib\n",
        "\n",
        "with open('data.jl', 'rb') as f:\n",
        "    data = joblib.load(f)\n",
        "cat_to_name = {f'{i}':j for i,j in enumerate(data.keys(),start=1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaS6DVajaMBb"
      },
      "source": [
        "len(cat_to_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpDTA3OJM7WB"
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 64\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "# specify data directory\n",
        "train = \"train/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yMGzf8bNDnc"
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ColorJitter(), #Randomly change the brightness, contrast and saturation of an image.\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])])\n",
        "\n",
        "# choose the training datasets\n",
        "train_data = datasets.ImageFolder(train, transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=64, \n",
        "    sampler=valid_sampler, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgcvQjkmPB5N"
      },
      "source": [
        "samples, labels = iter(train_loader).next()\n",
        "plt.figure(figsize=(16,24))\n",
        "grid_imgs = torchvision.utils.make_grid(samples[:24])\n",
        "np_grid_imgs = grid_imgs.numpy()\n",
        "# Transpose image from tensor format (batch, width, height) to numpy (width, height, batch) to show it.\n",
        "plt.imshow(np.transpose(np_grid_imgs, (1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aJTpjflPmQK"
      },
      "source": [
        "## **Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu015pkfPj7F"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=3, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        " \n",
        "    def __call__(self, val_loss, model):\n",
        " \n",
        "        score = -val_loss\n",
        " \n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        " \n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), \"models/\"+f\"bestmodel.pt\")\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JByvV27IQQuN"
      },
      "source": [
        "### **Transfer Learning: Resnext50_32x4d**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk-AQucxQO75"
      },
      "source": [
        "# Load a pretrained model\n",
        "model = models.resnext50_32x4d(pretrained=False)\n",
        "model.fc # Check resnet's fully connected layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2DYXjgyQPgY"
      },
      "source": [
        "model = models.resnext50_32x4d(pretrained=False)\n",
        " \n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "    \n",
        "# Get model Output Size = Number of Categories\n",
        "output_size = len(cat_to_name)\n",
        " \n",
        "# Input size from current classifier\n",
        "input_size = model.fc.in_features\n",
        " \n",
        "classifier = nn.Sequential(nn.Linear(input_size, 1024),\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.2),\n",
        "                           nn.Linear(1024, 256),\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.2),\n",
        "                           nn.Linear(256, 34),\n",
        "                          )\n",
        " \n",
        "model.fc = classifier\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "print(model)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.3, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=7, verbose=True)\n",
        " \n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXItuuYZRul_"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRgKuHLKRw3h"
      },
      "source": [
        "directory = 'models1/'\n",
        "epochs = 10\n",
        "train_loss_list, valid_loss_list = [], []\n",
        "valid_acc_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_counter = 0\n",
        "    for bi, (samples, labels) in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
        "        train_counter += 1\n",
        "        samples, labels = samples.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(samples)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "      \n",
        "    train_loss = train_loss/train_counter\n",
        "    train_loss_list.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_label, correct = 0, 0\n",
        "        samples, labels = iter(valid_loader).next()\n",
        "        samples, labels = samples.to(device), labels.to(device)\n",
        "        output = model(samples)\n",
        "        loss = criterion(output, labels)\n",
        "        valid_loss = loss.item()\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        total_label = labels.size(0)\n",
        "        correct = pred.eq(labels).sum().item()\n",
        "\n",
        "    valid_acc = (100 * correct) // total_label\n",
        "    valid_loss_list.append(valid_loss)\n",
        "    valid_acc_list.append(valid_acc)\n",
        "\n",
        "    print('[Epoch {}/{}] -> Train Loss: {:.4f} -> Valid Loss: {:.4f}, Valid Accuracy: {:.3f}%'.format(epoch+1, epochs, train_loss, valid_loss, valid_acc))\n",
        "\n",
        "    # Early Stopping\n",
        "    early_stopping(valid_loss, model)  \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping at {} epoch\".format(epoch))\n",
        "        break\n",
        "\n",
        "    scheduler.step(valid_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4F_Wq1gorir"
      },
      "source": [
        "train_data.class_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4dAYhrATNYT"
      },
      "source": [
        "plt.plot(train_loss_list, label = 'Training loss')\n",
        "plt.plot(valid_loss_list, label = 'Validation loss')\n",
        "plt.legend(frameon = False)\n",
        "plt.title('Training Loss vs Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV9aWz6YTbd9"
      },
      "source": [
        "def load_checkpoint(model, file=\"models/\"+f\"bestmodel.pt\"):\n",
        "    state_dict = torch.load(file)\n",
        "    model.load_state_dict(state_dict)\n",
        "    # model.load_state_dict(state_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "chkp_model = load_checkpoint(model)\n",
        "chkp_model = chkp_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8FA9MtKTxyS"
      },
      "source": [
        "### **Verifying Validation Accuracy and Error**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA42ihIzTxZ5"
      },
      "source": [
        "with torch.no_grad():\n",
        "    valid_loss = 0.0\n",
        "    chkp_model.eval()\n",
        "    total_label, correct = 0, 0\n",
        "    for samples, labels in valid_loader:\n",
        "        samples, labels = samples.to(device), labels.to(device)\n",
        "        output = chkp_model(samples)\n",
        "        loss = criterion(output, labels)\n",
        "        valid_loss += loss.item()*samples.size(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total_label += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    valid_acc = (100 * correct) // total_label\n",
        "\n",
        "print('Valid Loss: {:.4f}, Valid Accuracy: {:.3f}%'.format(valid_loss, valid_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnBn4wzAT-V6"
      },
      "source": [
        "### **View Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpeOFg1UEtV"
      },
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "idx_to_class = {class_to_idx[k]: k for k in class_to_idx}\n",
        "idx_to_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKBM_yZywx_3"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82a4swJothzk"
      },
      "source": [
        "model = models.resnext50_32x4d(pretrained=False)\n",
        "state_dict = torch.load('models/bestmodel.pt')\n",
        "classifier = nn.Sequential(nn.Linear(input_size, 1024),\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.2),\n",
        "                           nn.Linear(1024, 256),\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.2),\n",
        "                           nn.Linear(256, 34),\n",
        "                          )\n",
        "\n",
        "model.fc = classifier\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1rUAQn1ULlr"
      },
      "source": [
        "samples, _ = iter(valid_loader).next()\n",
        "samples = samples.to(device)\n",
        "fig = plt.figure(figsize=(24, 16))\n",
        "fig.tight_layout()\n",
        "output = chkp_model(samples[:24])\n",
        "pred = torch.argmax(output, dim=1)\n",
        "pred = [p.item() for p in pred]\n",
        "\n",
        "for num, sample in enumerate(samples[:24]):\n",
        "    plt.subplot(4,6,num+1);\n",
        "    plt.title(cat_to_name[idx_to_class[pred[num]]]);\n",
        "    plt.axis('off');\n",
        "    sample = sample.cpu().numpy();\n",
        "    plt.imshow(np.transpose(sample, (1,2,0)));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AR7GJlxpvkn"
      },
      "source": [
        "### **Future Revisions**\n",
        "*  Try other pretrained models\n",
        "*  KFold CV\n",
        "*  Remove the train data modifications\n"
      ]
    }
  ]
}